---
type: project
created: 2026-01-27
status: active
area: "[[AI]]"
tags: [learning, multimodal, CLIP, beginner]
---
# 七天入门跨模态学习

## Context（背景）

**目标：** 从零开始掌握跨模态图文检索的核心概念和实践技能

**成功标准：**
- [ ] 能独立解释 CLIP 的工作原理
- [ ] 能用代码实现一个简单的图文检索系统
- [ ] 理解对比学习的核心思想
- [ ] 完成一个小项目展示所学

---

## Actions（行动）

### Day 1 - 基础概念 ✅ (2026-01-27)

- [x] 完成 [[跨模态图文检索]] 研究笔记
- [x] 理解核心概念：[[对比学习]], [[CLIP模型]], [[多模态表示学习]]
- [ ] 阅读 CLIP 论文摘要和引言部分

**今日重点：** 建立整体认知框架

---

### Day 2 - 深入 CLIP 论文

- [ ] 阅读 CLIP 论文方法部分 ([论文链接](https://arxiv.org/abs/2103.00020))
- [ ] 理解双塔架构的设计思路
- [ ] 理解 InfoNCE 损失函数
- [ ] 用 `/ask` 问不懂的概念

**今日重点：** 理解"为什么这样设计"

---

### Day 3 - 动手实践（基础）

- [ ] 配置 Python 环境，安装 transformers, torch, PIL
- [ ] 运行 CLIP 官方示例代码
- [ ] 尝试不同的图片和文本组合
- [ ] 记录实验结果到日记

**今日重点：** 让代码跑起来

```bash
pip install transformers torch pillow requests
```

---

### Day 4 - 动手实践（进阶）

- [ ] 用自己的图片数据集测试
- [ ] 实现一个简单的"以文搜图"功能
- [ ] 尝试中文文本检索（CLIP 对中文支持有限，观察效果）
- [ ] 探索 Chinese-CLIP 模型

**今日重点：** 从"能跑"到"能用"

---

### Day 5 - 理解视觉编码器

- [ ] 学习 [[视觉编码器]] 和 ViT 基础
- [ ] 理解 Patch Embedding 的概念
- [ ] 用 `/research ViT` 深入研究
- [ ] 对比 ResNet 和 ViT 的区别

**今日重点：** 理解图像如何变成向量

---

### Day 6 - 扩展阅读

- [ ] 了解 BLIP-2 的改进（Q-Former 架构）
- [ ] 对比 CLIP、ALIGN、BLIP-2 的优缺点
- [ ] 探索多模态大模型的发展趋势
- [ ] 用 `/research 多模态大模型` 扩展知识

**今日重点：** 建立技术发展的全景图

---

### Day 7 - 总结与项目

- [ ] 完成一个迷你项目：个人图片搜索引擎
- [ ] 整理学习笔记，更新知识库
- [ ] 写一篇学习总结
- [ ] 规划下一步学习方向

**今日重点：** 用项目巩固所学

---

## Progress（进展）

### 2026-01-27

- 完成了跨模态图文检索的系统研究 [[2026-01-27]]
- 创建了 6 个知识库条目
- 熟悉了 OrbitOS 的基本使用

---

## 资源清单

**必读论文：**
- [CLIP 论文](https://arxiv.org/abs/2103.00020)
- [BLIP-2 论文](https://arxiv.org/abs/2301.12597)

**代码资源：**
- [HuggingFace CLIP](https://huggingface.co/openai/clip-vit-base-patch32)
- [Chinese-CLIP](https://github.com/OFA-Sys/Chinese-CLIP)

**视频教程（可选）：**
- 搜索 "CLIP 论文精读" 或 "李沐 CLIP"

---

## 相关笔记

- [[跨模态图文检索]]
- [[对比学习]]
- [[CLIP模型]]
- [[多模态表示学习]]
