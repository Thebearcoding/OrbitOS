---
type: reference
created: 2026-01-27
area: "[[AI]]"
tags: [research, cross-modal, image-text-retrieval, CLIP, multimodal]
status: complete
---
# 跨模态图文检索

## 概述

跨模态图文检索(Cross-modal Image-Text Retrieval)是一种多模态学习任务，旨在建立图像和文本之间的语义关联。其核心目标是：给定一张图像，检索与之语义相关的文本描述；或给定一段文本，检索与之匹配的图像。

这项技术的关键挑战在于**模态鸿沟(Modality Gap)**——图像和文本具有完全不同的数据形式和特征空间，如何将它们映射到统一的语义空间是核心问题。

**应用场景：**
- 图像搜索引擎（以文搜图）
- 电商商品检索
- 多媒体内容推荐
- 视觉问答系统
- 图像描述生成

## 核心概念

跨模态图文检索涉及以下核心概念：

- [[对比学习]] - 通过正负样本对比来学习表示
- [[多模态表示学习]] - 学习跨模态的统一表示空间
- [[CLIP模型]] - OpenAI的视觉-语言预训练模型
- [[视觉编码器]] - 提取图像特征的神经网络
- [[文本编码器]] - 提取文本特征的神经网络
- [[零样本学习]] - 无需特定任务训练即可泛化

## 工作原理

### 双塔架构(Dual-Encoder Architecture)

跨模态图文检索的主流方法采用双塔架构：

```
图像 → [视觉编码器] → 图像嵌入
                              ↘
                                相似度计算 → 匹配分数
                              ↗
文本 → [文本编码器] → 文本嵌入
```

**核心步骤：**

1. **特征提取**：分别使用视觉编码器(如 ViT)和文本编码器(如 Transformer)提取图像和文本的高维特征
2. **投影对齐**：通过投影层将两种模态的特征映射到相同维度的共享语义空间
3. **相似度计算**：使用余弦相似度或点积计算图文匹配分数
4. **对比学习训练**：通过 InfoNCE 损失函数拉近正样本对、推开负样本对

### InfoNCE 损失函数

对比学习的核心损失函数：

$$L = -\log \frac{\exp(sim(v_i, t_i)/\tau)}{\sum_{j=1}^{N} \exp(sim(v_i, t_j)/\tau)}$$

其中：
- $v_i, t_i$ 是匹配的图文嵌入对
- $\tau$ 是温度参数，控制分布的平滑程度
- $N$ 是批次大小

### 批次大小的重要性

大批次训练对对比学习至关重要：
- 提供更多负样本，增强模型区分能力
- CLIP 使用 32,768 的批次大小
- 小批次可能导致模型过拟合或学习到虚假关联

## 主流模型

### CLIP (Contrastive Language-Image Pre-training)

**OpenAI, 2021**

CLIP 是跨模态图文检索的里程碑模型，通过在 4 亿图文对上进行对比学习预训练。

**架构：**
- 视觉编码器：ViT (Vision Transformer) 或 ResNet
- 文本编码器：Transformer
- 投影维度：512

**特点：**
- 强大的零样本迁移能力
- 开放词汇图像分类
- 无需微调即可应用于下游任务

**代码示例：**

```python
import torch
from transformers import AutoProcessor, CLIPModel

# 加载模型
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = AutoProcessor.from_pretrained("openai/clip-vit-base-patch32")

# 准备输入
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
from PIL import Image
import requests
image = Image.open(requests.get(url, stream=True).raw)

labels = ["一只猫的照片", "一只狗的照片", "一辆汽车的照片"]
inputs = processor(text=labels, images=image, return_tensors="pt", padding=True)

# 推理
with torch.inference_mode():
    outputs = model(**inputs)
    logits_per_image = outputs.logits_per_image
    probs = logits_per_image.softmax(dim=1)

print(f"预测结果: {labels[probs.argmax().item()]}")
```

### ALIGN (A Large-scale ImaGe and Noisy-text embedding)

**Google, 2021**

ALIGN 采用与 CLIP 相似的双编码器架构，但使用了超过 10 亿的噪声图文对进行训练。

**特点：**
- 强调数据规模而非数据质量
- 使用 EfficientNet 作为视觉编码器
- 证明了噪声数据的有效性

**与 CLIP 的区别：**
- 数据量更大（10亿 vs 4亿）
- 不需要精细的数据清洗
- 计算成本更高

### BLIP-2 (Bootstrapping Language-Image Pre-training 2)

**Salesforce, 2023**

BLIP-2 引入了 Q-Former (Querying Transformer)，在冻结的视觉编码器和语言模型之间搭建桥梁。

**架构：**
- 冻结的视觉编码器（如 ViT-G）
- Q-Former：轻量级 Transformer（32 个可学习查询）
- 冻结的大语言模型（如 OPT、Flan-T5）

**两阶段训练：**
1. **视觉-语言表示学习**：从冻结的图像编码器学习视觉表示
2. **视觉-语言生成学习**：从冻结的语言模型学习生成能力

**代码示例：**

```python
import torch
from transformers import Blip2Processor, Blip2ForConditionalGeneration
from PIL import Image
import requests

# 加载模型
processor = Blip2Processor.from_pretrained("Salesforce/blip2-opt-2.7b")
model = Blip2ForConditionalGeneration.from_pretrained(
    "Salesforce/blip2-opt-2.7b",
    torch_dtype=torch.float16
)

# 图像描述生成
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(images=image, return_tensors="pt").to(torch.float16)
generated_ids = model.generate(**inputs)
generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(f"图像描述: {generated_text}")

# 视觉问答
prompt = "Question: how many cats are there? Answer:"
inputs = processor(images=image, text=prompt, return_tensors="pt").to(torch.float16)
generated_ids = model.generate(**inputs)
answer = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(f"答案: {answer}")
```

### 模型对比

| 模型 | 数据规模 | 视觉编码器 | 文本编码器 | 特点 |
|------|----------|------------|------------|------|
| CLIP | 4亿对 | ViT/ResNet | Transformer | 强零样本能力 |
| ALIGN | 18亿对 | EfficientNet | BERT | 大规模噪声数据 |
| BLIP-2 | - | 冻结ViT | 冻结LLM+Q-Former | 高效参数使用 |

## 示例

### 使用 CLIP 进行图文检索

```python
import torch
from transformers import CLIPModel, CLIPProcessor
from PIL import Image
import numpy as np

class ImageTextRetriever:
    def __init__(self, model_name="openai/clip-vit-base-patch32"):
        self.model = CLIPModel.from_pretrained(model_name)
        self.processor = CLIPProcessor.from_pretrained(model_name)
        self.model.eval()

    def encode_images(self, images):
        """编码图像列表"""
        inputs = self.processor(images=images, return_tensors="pt")
        with torch.inference_mode():
            image_features = self.model.get_image_features(**inputs)
        return image_features / image_features.norm(dim=-1, keepdim=True)

    def encode_texts(self, texts):
        """编码文本列表"""
        inputs = self.processor(text=texts, return_tensors="pt", padding=True)
        with torch.inference_mode():
            text_features = self.model.get_text_features(**inputs)
        return text_features / text_features.norm(dim=-1, keepdim=True)

    def text_to_image_search(self, query_text, image_embeddings, top_k=5):
        """以文搜图"""
        text_embedding = self.encode_texts([query_text])
        similarities = (text_embedding @ image_embeddings.T).squeeze()
        top_indices = similarities.argsort(descending=True)[:top_k]
        return top_indices, similarities[top_indices]

    def image_to_text_search(self, query_image, text_embeddings, top_k=5):
        """以图搜文"""
        image_embedding = self.encode_images([query_image])
        similarities = (image_embedding @ text_embeddings.T).squeeze()
        top_indices = similarities.argsort(descending=True)[:top_k]
        return top_indices, similarities[top_indices]

# 使用示例
retriever = ImageTextRetriever()

# 构建图像索引
images = [...]  # 图像列表
image_embeddings = retriever.encode_images(images)

# 以文搜图
query = "一只橘猫在沙发上睡觉"
indices, scores = retriever.text_to_image_search(query, image_embeddings)
print(f"最相关的图像索引: {indices.tolist()}")
```

### 使用 BLIP-2 进行图文匹配

```python
import torch
from transformers import Blip2Processor, Blip2ForImageTextRetrieval
from PIL import Image

# 加载模型
processor = Blip2Processor.from_pretrained("Salesforce/blip2-itm-vit-g")
model = Blip2ForImageTextRetrieval.from_pretrained(
    "Salesforce/blip2-itm-vit-g",
    torch_dtype=torch.float16
)

# 准备输入
image = Image.open("cat.jpg")
text = "两只猫躺在粉色毯子上"

inputs = processor(images=image, text=text, return_tensors="pt").to(torch.float16)

# 图文匹配（ITM）
itm_out = model(**inputs, use_image_text_matching_head=True)
probs = itm_out.logits_per_image.softmax(dim=1)
print(f"匹配概率: {probs[0][1]:.1%}")

# 对比检索（ITC）
texts = ["一只猫的照片", "一只狗的照片", "一辆车的照片"]
inputs = processor(images=image, text=texts, return_tensors="pt").to(torch.float16)
itc_out = model(**inputs, use_image_text_matching_head=False)
probs = itc_out.logits_per_image.softmax(dim=1)
for i, t in enumerate(texts):
    print(f"{t}: {probs[0][i]:.1%}")
```

## 最佳实践

### 1. 数据准备

- **高质量图文对**：确保图像和文本描述语义匹配
- **多样性**：覆盖不同场景、物体、动作
- **数据增强**：对图像进行裁剪、翻转等增强

### 2. 模型选择

- **通用检索**：CLIP（平衡性能和效率）
- **高精度需求**：BLIP-2（更强的理解能力）
- **大规模部署**：考虑模型蒸馏或量化

### 3. 嵌入索引

- 使用 FAISS 等向量数据库加速检索
- 对嵌入进行 L2 归一化
- 考虑使用近似最近邻(ANN)算法

### 4. 推理优化

```python
# 使用 ONNX 加速
from optimum.onnxruntime import ORTModelForFeatureExtraction

model = ORTModelForFeatureExtraction.from_pretrained(
    "openai/clip-vit-base-patch32",
    export=True
)

# 使用半精度
model = model.half().cuda()

# 批量处理
batch_size = 32
for i in range(0, len(images), batch_size):
    batch = images[i:i+batch_size]
    embeddings = encode_images(batch)
```

## 常见陷阱

### 1. 批次大小过小

对比学习依赖大量负样本。批次过小会导致：
- 模型收敛困难
- 学习到的表示质量差
- 容易过拟合

**解决方案**：使用梯度累积、内存队列或跨 GPU 同步

### 2. 温度参数不当

- 温度过高：分布过于平滑，难以区分正负样本
- 温度过低：分布过于尖锐，训练不稳定

**建议**：从 0.07 开始，根据验证集调整

### 3. 模态不平衡

图像和文本的信息量可能不对等：
- 短文本可能无法完整描述图像
- 复杂图像可能需要详细描述

**解决方案**：使用多个文本描述、硬负样本挖掘

### 4. 领域偏移

预训练模型可能不适应特定领域：
- 医学图像
- 遥感图像
- 专业术语

**解决方案**：在目标领域进行微调

### 5. 评估指标选择

不同场景需要不同指标：
- **Recall@K**：检索任务
- **MRR**：排序质量
- **mAP**：整体精度

## 相关阅读

- [[对比学习]] - 理解对比学习的基本原理
- [[多模态表示学习]] - 多模态融合的更多方法
- [[CLIP模型]] - CLIP的详细介绍
- [[视觉-语言预训练]] - VLP模型的发展历程
- [[Transformer]] - 理解编码器架构
- [[ViT]] - Vision Transformer 详解

## 参考资源

### 论文

1. [CLIP: Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) - Radford et al., 2021
2. [ALIGN: Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision](https://arxiv.org/abs/2102.05918) - Jia et al., 2021
3. [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597) - Li et al., 2023

### 代码与教程

- [HuggingFace CLIP 文档](https://huggingface.co/docs/transformers/model_doc/clip)
- [HuggingFace BLIP-2 文档](https://huggingface.co/docs/transformers/model_doc/blip-2)
- [OpenAI CLIP GitHub](https://github.com/openai/CLIP)

### 预训练模型

- [openai/clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32)
- [Salesforce/blip2-opt-2.7b](https://huggingface.co/Salesforce/blip2-opt-2.7b)
- [Salesforce/blip2-itm-vit-g](https://huggingface.co/Salesforce/blip2-itm-vit-g)
