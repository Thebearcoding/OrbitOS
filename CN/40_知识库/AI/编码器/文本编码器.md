---
area: "[[AI]]"
tags: [neural-network, nlp, encoder]
created: 2026-01-27
---
# 文本编码器

## 定义

文本编码器(Text Encoder)是将自然语言文本转换为高维特征向量的神经网络模块。它是自然语言处理和多模态系统的核心组件，负责捕捉文本的语义信息。

## 要点

- **Transformer 架构**：BERT、GPT 等，通过自注意力机制捕捉上下文关系
- **分词器**：将文本切分为 token 序列，常用 BPE (Byte-Pair Encoding)
- **输出形式**：句子级嵌入(CLS token/pooled)或 token 级嵌入序列
- **最大长度**：CLIP 文本编码器限制为 77 tokens

## 示例

常见的文本编码器及其应用：

| 模型 | 架构 | 参数量 | 应用 |
|------|------|--------|------|
| BERT-base | Encoder | 110M | 文本理解 |
| GPT-2 | Decoder | 117M | 文本生成 |
| CLIP Text | Transformer | 63M | 多模态对齐 |

在 CLIP 中使用文本编码器：

```python
from transformers import CLIPTextModel, CLIPTokenizer

tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")
text_model = CLIPTextModel.from_pretrained("openai/clip-vit-base-patch32")

# 文本编码
texts = ["一只可爱的猫", "一只忠诚的狗"]
inputs = tokenizer(texts, return_tensors="pt", padding=True)

# 输出: last_hidden_state [batch, seq_len, 512], pooler_output [batch, 512]
outputs = text_model(**inputs)
text_features = outputs.pooler_output  # 句子级特征
```

**注意事项：**
- 文本长度超过最大限制会被截断
- 不同模型使用不同的分词器，需要配套使用
- 句子级嵌入通常取 [CLS] token 或 [EOS] token 的输出

## 相关概念

- [[视觉编码器]] - 图像模态的对应组件
- [[Transformer]] - 文本编码器的基础架构
- [[CLIP模型]] - 使用文本编码器的多模态模型
- [[多模态表示学习]] - 文本编码器的应用场景

## 参考资料

- [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
